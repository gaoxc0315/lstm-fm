{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/gaoxc/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............test_value shape is (1, 10, 24)\n",
      "<class 'list'>\n",
      "(590, 10, 24)\n",
      "feat_value shape is (?, 24)\n",
      "feat_index shape is  (?, 24)\n",
      "embedding_part shape is (?, 24, 5)\n",
      "embedding_part: Tensor(\"sec_lstm/Mul:0\", shape=(?, 24, 5), dtype=float32)\n",
      "embedding_first shape is (?, 24, 1)\n",
      "first_order: (?, 24)\n",
      "sum_square_second_order: Tensor(\"sec_lstm/Square:0\", shape=(?, 5), dtype=float32)\n",
      "square_sum_second_order: Tensor(\"sec_lstm/Sum_2:0\", shape=(?, 5), dtype=float32)\n",
      "fm_part: Tensor(\"sec_lstm/concat:0\", shape=(?, 29), dtype=float32)\n",
      "fm_part shape is (?, 10, 29)\n",
      "embedding shape is (?, 10, 120)\n",
      "continuous_part shape is (?, 10, 19)\n",
      ".........din_all shape is (?, 10, 168)\n",
      "..........din_all shape is (?, 10, 168)\n",
      "WARNING:tensorflow:From <ipython-input-1-0d78de6f0ed6>:215: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-0d78de6f0ed6>:223: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda540c29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda540c29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda540c29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda540c29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "(55, 10, 5)\n",
      ".......output shape (55, 5)\n",
      ":::::::::final_states shape is LSTMStateTuple(c=<tf.Tensor 'sec_lstm/rnn/while/Exit_3:0' shape=(55, 5) dtype=float32>, h=<tf.Tensor 'sec_lstm/rnn/while/Exit_4:0' shape=(55, 5) dtype=float32>)\n",
      "----w_out shape is (5, 1)\n",
      "----b_out shape is (1,)\n",
      "............weights out shape is (5, 1)\n",
      "------pred shape is  (55, 1)\n",
      ".......loss ()\n",
      ".......loss ()\n",
      "0 0.15202598\n",
      "1 0.027855374\n",
      "2 0.020780519\n",
      "3 0.012039938\n",
      "4 0.008758754\n",
      "5 0.004854136\n",
      "6 0.0031111573\n",
      "7 0.0025869824\n",
      "8 0.0020096526\n",
      "9 0.0015763871\n",
      "10 0.0013632715\n",
      "11 0.0011342533\n",
      "12 0.0009869587\n",
      "13 0.00084372517\n",
      "14 0.00078893075\n",
      "15 0.00070082216\n",
      "16 0.0006011833\n",
      "17 0.00058343617\n",
      "18 0.0005235785\n",
      "19 0.0005353136\n",
      "20 0.00052509014\n",
      "21 0.0005211614\n",
      "22 0.0004964854\n",
      "23 0.00038567002\n",
      "24 0.00039509265\n",
      "25 0.0003141404\n",
      "26 0.00043971697\n",
      "27 0.00058431807\n",
      "28 0.0005374478\n",
      "29 0.000770447\n",
      "30 0.0005040699\n",
      "31 0.0007146315\n",
      "32 0.00042249248\n",
      "33 0.00043017886\n",
      "34 0.0005563037\n",
      "35 0.00055981416\n",
      "36 0.0012960193\n",
      "37 0.0012985299\n",
      "38 0.00071067933\n",
      "39 0.001717387\n",
      "40 0.0026611076\n",
      "41 0.0035654919\n",
      "42 0.002619828\n",
      "43 0.0024129634\n",
      "44 0.0040429137\n",
      "45 0.002920864\n",
      "46 0.0012322464\n",
      "47 0.00064928515\n",
      "48 0.0007659724\n",
      "49 0.0009895387\n",
      "保存模型： model_save1/modle.ckpt\n",
      "Well Done!\n",
      "feat_value shape is (?, 24)\n",
      "feat_index shape is  (?, 24)\n",
      "embedding_part shape is (?, 24, 5)\n",
      "embedding_part: Tensor(\"sec_lstm_1/Mul:0\", shape=(?, 24, 5), dtype=float32)\n",
      "embedding_first shape is (?, 24, 1)\n",
      "first_order: (?, 24)\n",
      "sum_square_second_order: Tensor(\"sec_lstm_1/Square:0\", shape=(?, 5), dtype=float32)\n",
      "square_sum_second_order: Tensor(\"sec_lstm_1/Sum_2:0\", shape=(?, 5), dtype=float32)\n",
      "fm_part: Tensor(\"sec_lstm_1/concat:0\", shape=(?, 29), dtype=float32)\n",
      "fm_part shape is (?, 10, 29)\n",
      "embedding shape is (?, 10, 120)\n",
      "continuous_part shape is (?, 10, 19)\n",
      ".........din_all shape is (?, 10, 168)\n",
      "..........din_all shape is (?, 10, 168)\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda446ff080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda446ff080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda446ff080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda446ff080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "(1, 10, 5)\n",
      ".......output shape (1, 5)\n",
      ":::::::::final_states shape is LSTMStateTuple(c=<tf.Tensor 'sec_lstm_1/rnn/while/Exit_3:0' shape=(1, 5) dtype=float32>, h=<tf.Tensor 'sec_lstm_1/rnn/while/Exit_4:0' shape=(1, 5) dtype=float32>)\n",
      "----w_out shape is (5, 1)\n",
      "----b_out shape is (1,)\n",
      "............weights out shape is (5, 1)\n",
      "------pred shape is  (1, 1)\n",
      "WARNING:tensorflow:From /home/gaoxc/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from model_save1/modle.ckpt\n",
      "[array([0.00140312], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def gen_train_data(feature_df):\n",
    "    \"\"\"\n",
    "    把数据加工成\n",
    "    feature_index：存放数据的位置，分类变量改成one_hot column 展开后index\n",
    "    feature_value：存放数据值（分类变量改成1）\n",
    "    :param feature_df:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_data = {}\n",
    "    target_temp = np.log1p(feature_df[label_column].values)\n",
    "    target_temp_nan = np.isnan(target_temp)\n",
    "\n",
    "    target_temp[target_temp_nan] = 0\n",
    "    ## 直接取对数后有的目标值就变成了nan，这里提前处理下nan值\n",
    "    train_data['y_train'] = target_temp  # np.log1p(feature_df[label_column].values)\n",
    "    #     train_data =\n",
    "    # train_data['y_train'] = (feature_df[['sum_grp_lmt_sale_amt']]-feature_df[['sum_grp_lmt_sale_amt']].mean())/feature_df[['sum_grp_lmt_sale_amt']].std()\n",
    "    # 连续变量df\n",
    "    co_feature = feature_df[co_feature_column]\n",
    "    # 离线变量df\n",
    "    ca_feature = feature_df[ca_feature_column]\n",
    "    co_feature = (co_feature - co_feature.mean()) / co_feature.std()\n",
    "    lbd = defaultdict(LabelEncoder)\n",
    "    ca_feature = ca_feature.apply(lambda x: lbd[x.name].fit_transform(x))\n",
    "\n",
    "    feature_value = pd.concat([co_feature, ca_feature], axis=1)\n",
    "    feature_index = feature_value.copy()\n",
    "    # feature_index 从1开始\n",
    "    # columns_index=list(zip(feature_value.columns,range(1,len(feature_value.columns)+1)))\n",
    "    cnt = 1\n",
    "    for c in feature_value.columns:\n",
    "        if c in co_feature_column:\n",
    "            feature_index[c] = cnt\n",
    "            cnt += 1\n",
    "        else:\n",
    "            feature_index[c] += cnt\n",
    "            feature_value[c] = 1\n",
    "            cnt += lbd[c].classes_.shape[0]\n",
    "    # feature_index是特征的一个序号，主要用于通过embedding_lookup选择我们的embedding\n",
    "    train_data['xi'] = feature_index.values\n",
    "    # feature_value是对应的特征值，如果是离散特征的话，就是1，如果不是离散特征的话，就保留原来的特征值。\n",
    "    train_data['xv'] = feature_value.fillna(0).values\n",
    "    train_data['feat_dim'] = cnt\n",
    "    train_data['xc'] = co_feature.fillna(0).values\n",
    "    # train_data['y_train']=np.ones((len(feature_df),1))\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def to_supervised(data_dict, n_input, n_out, batch_sizes):\n",
    "    #     data = train\n",
    "    data_train = data_dict['xv']\n",
    "    data_index = data_dict['xi']\n",
    "    data_y = data_dict['y_train']\n",
    "    data_con = data_dict['xc']\n",
    "    X_data, X_index, y, X_continuous = list(), list(), list(), list()\n",
    "    in_start = 0\n",
    "    batch_index = []\n",
    "    for i in range(len(data_train)):\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        if out_end <= len(data_train):\n",
    "            X_data.append(data_train[in_start:in_end, :])  # 使用几个特征\n",
    "            X_index.append(data_index[in_start:in_end, :])\n",
    "            y.append(data_y[in_end:out_end, 0])\n",
    "            X_continuous.append(data_con[in_start:in_end, :])\n",
    "            if i % batch_sizes == 0:\n",
    "                batch_index.append(i)\n",
    "        in_start += 1\n",
    "    return np.array(X_data), np.array(X_index, dtype=np.int32), np.array(y), np.array(X_continuous), batch_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_sizes = 55\n",
    "time_steps = 10\n",
    "con_features_size = 19\n",
    "ca_features_size = 5\n",
    "feature_sizes = con_features_size + ca_features_size\n",
    "embedding_size = 5\n",
    "rnn_unit = 5\n",
    "epoch = 50\n",
    "\n",
    "# 生成模型使用的数据\n",
    "\n",
    "\n",
    "co_feature_column = list('abcdefghijklmnopqrs')\n",
    "ca_feature_column = list('vwxyz')\n",
    "\n",
    "df_co = pd.DataFrame(np.random.normal(0, 1, size=(600, len(co_feature_column))), columns=co_feature_column)\n",
    "df_ca = pd.DataFrame(np.random.randint(1, 30, size=(600, len(ca_feature_column))), columns=ca_feature_column)\n",
    "# pd.read_csv('./lstmfm_data.csv', sep='\\t').fillna(0)\n",
    "df = pd.concat([df_co, df_ca], axis=1)\n",
    "df['target'] = 0\n",
    "label_column = ['target']\n",
    "data_dict = gen_train_data(df)\n",
    "\n",
    "n_dim = data_dict['feat_dim']\n",
    "\n",
    "data_train_, data_index_, data_y_, data_con_, batch_index_ = to_supervised(data_dict,\n",
    "                                                                           10, 1,\n",
    "                                                                           80)\n",
    "# data_dict = train_data\n",
    "\n",
    "# 用于lstmfm最后一层的输入，当作全连接层\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([rnn_unit, 1])),\n",
    "    'feature_weight': tf.Variable(tf.random_normal([n_dim, embedding_size], 0, 1.0),\n",
    "                                  name='feature_weight'),\n",
    "    'feature_first': tf.Variable(tf.random_normal([n_dim, 1], 0, 1.0), name='feature_first')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[1, ]))\n",
    "}\n",
    "\n",
    "feat_index = tf.placeholder(tf.int32, shape=[None, time_steps, feature_sizes],\n",
    "                                 name='feature_index')\n",
    "feat_value = tf.placeholder(tf.float32, shape=[None, time_steps, feature_sizes],\n",
    "                                 name='feature_value')\n",
    "label = tf.placeholder(tf.float32, shape=[None, None, None], name='label')\n",
    "continuous_part = tf.placeholder(tf.float32, shape=[None, time_steps, con_features_size],\n",
    "                                      name='continuous_feautre')\n",
    "\n",
    "\n",
    "\n",
    "def build_model(batch_size_changes):\n",
    "    # def lstmfm():\n",
    "    \"\"\"\n",
    "    构建网络结构 lstmfm\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # One-hot编码后的输入层与Dense embeddings层的权值定义，即DNN的输入embedding。\n",
    "    # 注：Dense embeddings层的神经元个数由field_size和决定\n",
    "    # （61*256）\n",
    "    #         weights['feature_weight'] = tf.Variable(tf.random_normal([n_dim, embedding_size], 0, 1),\n",
    "    #                                                name='feature_weight')\n",
    "\n",
    "    #         # FM部分中一次项的权值定义\n",
    "    #         # shape (61,1)\n",
    "    #         weights['feature_first'] = tf.Variable(tf.random_normal([n_dim, 1], 0, 1.0),\n",
    "    #                                               name='feature_first')\n",
    "\n",
    "    # embedding_part\n",
    "    # shape (?,39,256) 当前特征的权重\n",
    "\n",
    "    feat_index_ = tf.reshape(feat_index, [-1, feature_sizes])\n",
    "    feat_value_ = tf.reshape(feat_value, [-1, feature_sizes])\n",
    "    print(\"feat_value shape is\", feat_value_.shape)\n",
    "    print(\"feat_index shape is \", feat_index_.shape)\n",
    "\n",
    "    embedding_index = tf.nn.embedding_lookup(weights['feature_weight'], feat_index_)  # Batch*F*K\n",
    "    print('embedding_part shape is', embedding_index.shape)\n",
    "\n",
    "    # shape (?,39,256)  BFK * BF1=BFK\n",
    "    embedding_part = tf.multiply(embedding_index, tf.reshape(feat_value_, [-1, feature_sizes, 1]))\n",
    "    # [Batch*F*1] * [Batch*F*K] = [Batch*F*K],用到了broadcast的属性\n",
    "    print('embedding_part:', embedding_part)\n",
    "\n",
    "    \"\"\"\n",
    "    网络传递结构\n",
    "    \"\"\"\n",
    "    # FM部分\n",
    "    # 一阶特征\n",
    "    # shape (?,39,1)\n",
    "\n",
    "    embedding_first = tf.nn.embedding_lookup(weights['feature_first'], feat_index_)  # bacth*F*1\n",
    "    embedding_first = tf.multiply(tf.reshape(feat_value_, [-1, feature_sizes, 1]), embedding_first)\n",
    "    print(\"embedding_first shape is\", embedding_first.shape)\n",
    "    # shape （？,39）\n",
    "    first_order = tf.reduce_sum(embedding_first, 2)\n",
    "    print('first_order:', first_order.shape)\n",
    "\n",
    "    # 二阶特征 和的平方-平方的和\n",
    "    sum_second_order = tf.reduce_sum(embedding_part, 1)\n",
    "    sum_second_order_square = tf.square(sum_second_order)\n",
    "    print('sum_square_second_order:', sum_second_order_square)\n",
    "\n",
    "    square_second_order = tf.square(embedding_part)\n",
    "    square_second_order_sum = tf.reduce_sum(square_second_order, 1)\n",
    "    print('square_sum_second_order:', square_second_order_sum)\n",
    "    ## 嵌入的部分特征\n",
    "    # 1/2*((a+b)^2 - a^2 - b^2)=ab\n",
    "    second_order = 0.5 * tf.subtract(sum_second_order_square, square_second_order_sum)\n",
    "\n",
    "    # FM部分的输出(39+256) ：用的是concat\n",
    "    fm_part = tf.concat([first_order, second_order], axis=1)\n",
    "    print('fm_part:', fm_part)\n",
    "    fm_part = tf.reshape(fm_part, [-1, time_steps, feature_sizes + embedding_size])\n",
    "    embedding_part = tf.reshape(embedding_part, [-1, time_steps, embedding_size * feature_sizes])\n",
    "    print(\"fm_part shape is\", fm_part.shape)\n",
    "    print(\"embedding shape is\", embedding_part.shape)\n",
    "    print(\"continuous_part shape is\", continuous_part.shape)\n",
    "\n",
    "    din_all = tf.concat([fm_part, embedding_part, continuous_part], axis=2)\n",
    "    print(\".........din_all shape is\", din_all.shape)\n",
    "    print(\"..........din_all shape is\", din_all.shape)\n",
    "\n",
    "    # model = Sequential()\n",
    "    # model.add(LSTM(rnn_unit, input_shape=(time_steps, din_all.shape[0])))\n",
    "    # model.summary()\n",
    "\n",
    "    # 改用lstm模型\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    # init_state = cell.zero_state(batch_sizes, dtype=tf.float32)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size_changes,\n",
    "                                 dtype=tf.float32)  #########################################################################\n",
    "    output_rnn, final_states = tf.nn.dynamic_rnn(cell, \n",
    "                                                 din_all, \n",
    "                                                 initial_state=init_state,\n",
    "                                                 dtype=tf.float32)  # output_rnn是记录lstm每个输出节点的结果，final_states是最后一个cell的结果\n",
    "    print(output_rnn.shape)\n",
    "    output_temp = tf.reshape(output_rnn, [-1, time_steps, rnn_unit])  # 作为输出层的输入\n",
    "    output = output_temp[:, -1, :]\n",
    "\n",
    "    print(\".......output shape\", output.shape)\n",
    "    print(\":::::::::final_states shape is\", final_states)\n",
    "\n",
    "    w_out = weights['out']\n",
    "    b_out = biases['out']\n",
    "    print(\"----w_out shape is\", w_out.shape)\n",
    "    print(\"----b_out shape is\", b_out.shape)\n",
    "    print(\"............weights out shape is\", weights['out'].shape)\n",
    "    pred = tf.matmul(output, w_out) + b_out\n",
    "\n",
    "    print(\"------pred shape is \", pred.shape)\n",
    "\n",
    "    #     return pred, final_states\n",
    "    # loss部分\n",
    "    # out = tf.nn.sigmoid(out)\n",
    "    #\n",
    "    # loss = -tf.reduce_mean(\n",
    "    #     label * tf.log(out + 1e-24) + (1 - label) * tf.log(1 - out + 1e-24))\n",
    "    # 改成平方差损失函数\n",
    "    # loss = tf.reduce_mean((out - label) ** 2)\n",
    "    #     loss = tf.nn.l2_loss(tf.subtract(label, out))\n",
    "\n",
    "    # 正则：sum(w^2)/2*l2_reg_rate\n",
    "    # 这边只加了weight，有需要的可以加上bias部分\n",
    "    # loss += tf.contrib.layers.l2_regularizer(l2_reg_rate)(weight[\"last_layer\"])\n",
    "\n",
    "    #     global_step = tf.Variable(0, trainable=False)\n",
    "    # opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # trainable_params = tf.trainable_variables()\n",
    "    # print(trainable_params)\n",
    "    # gradients = tf.gradients(loss, trainable_params)\n",
    "    # clip_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
    "    # train_op = opt.apply_gradients(\n",
    "    #     zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    # optimizer\n",
    "    #     if optimizer_type == \"adam\":\n",
    "    #         optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999,\n",
    "    #                                                 epsilon=1e-8).minimize(loss)\n",
    "    #     elif optimizer_type == \"adagrad\":\n",
    "    #         optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate,\n",
    "    #                                                    initial_accumulator_value=1e-8).minimize(loss)\n",
    "    #     elif optimizer_type == \"gd\":\n",
    "    #         optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    #     elif optimizer_type == \"momentum\":\n",
    "    #         optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.95).minimize(\n",
    "    #             loss)\n",
    "    return pred\n",
    "\n",
    "def train():\n",
    "    global batch_sizes\n",
    "    data_train, data_index, data_y, data_con, batch_index = to_supervised(data_dict,\n",
    "                                                                          time_steps, 1,\n",
    "                                                                          batch_sizes)\n",
    "    print(type(batch_index))\n",
    "    print(np.array(data_train).shape)  # 3785  15  7\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred = build_model(batch_sizes)\n",
    "    loss = tf.reduce_mean(tf.square(tf.reshape(pred, [-1]) - tf.reshape(label, [-1])))\n",
    "    \n",
    "    print(\".......loss\",loss.shape)\n",
    "    print(\".......loss\",loss.shape)\n",
    "    train_op = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # 重复训练200次\n",
    "        for i in range(epoch):  # epoch\n",
    "            # 每次进行训练的时候，每个batch训练batch_sizes个样本\n",
    "            for step in range(len(batch_index) - 1):\n",
    "                # print(data_train[batch_index[step]:batch_index[step + 1]])\n",
    "                _, loss_ = sess.run([train_op, loss],\n",
    "                                    feed_dict={feat_value: data_train[batch_index[step]:batch_index[step + 1]],\n",
    "                                               feat_index: data_index[batch_index[step]:batch_index[step + 1]],\n",
    "                                               continuous_part: data_con[batch_index[step]:batch_index[step + 1]],\n",
    "                                               label: np.array(\n",
    "                                                   data_y[batch_index[step]:batch_index[step + 1]]).reshape(-1,\n",
    "                                                                                                            batch_sizes,\n",
    "                                                                                                            1)})\n",
    "            print(i, loss_)\n",
    "\n",
    "            \n",
    "        print(\"保存模型：\", saver.save(sess, 'model_save1/modle.ckpt'))\n",
    "        print(\"Well Done!\")\n",
    "    # pred_ = sess.run([pred],\n",
    "    #          feed_dict={feat_value: data_train,\n",
    "    #                     feat_index: data_index,\n",
    "    #                     continuous_part: data_con\n",
    "    #                     }\n",
    "    #          )\n",
    "    \n",
    "test_value = data_dict['xv'][0:10, 0:24].reshape(1, 10, 24)\n",
    "print(\"...............test_value shape is\", test_value.shape)\n",
    "test_index = data_dict['xi'][0:10, 0:24].reshape(1, 10, 24)\n",
    "con_values = data_dict['xc'][0:10, :].reshape(1, 10, 19)\n",
    "def prediction(test_value,test_index,con_values):\n",
    "    with tf.variable_scope(\"sec_lstm\", reuse=tf.AUTO_REUSE):\n",
    "        pred= build_model(1)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        saver.restore(sess, 'model_save1/modle.ckpt')\n",
    "        # I run the code in windows 10,so use  'model_save1\\\\modle.ckpt'\n",
    "        # if you run it in Linux,please use  'model_save1/modle.ckpt'\n",
    "        predict = []\n",
    "\n",
    "        next_seq = sess.run(pred, feed_dict={feat_value: test_value,\n",
    "                                       feat_index: test_index,\n",
    "                                       continuous_part: con_values\n",
    "                                       })\n",
    "        predict.append(next_seq[-1])\n",
    "        print(predict)\n",
    "#         plt.figure()\n",
    "#         plt.plot(list(range(len(data))),data, color='b')\n",
    "#         plt.plot(list(range(time_step + 1, np.shape(train_x)[0] + 1 + time_step)), [ value*s+m for value in predict], color='r')\n",
    "#         plt.show()\n",
    "\n",
    "# prediction()\n",
    "\n",
    "\n",
    "# 加工训练数据\n",
    "\n",
    "\n",
    "\n",
    "train()\n",
    "\n",
    "prediction(test_value,test_index,con_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_value shape is (?, 24)\n",
      "feat_index shape is  (?, 24)\n",
      "embedding_part shape is (?, 24, 5)\n",
      "embedding_part: Tensor(\"sec_lstm_2/Mul:0\", shape=(?, 24, 5), dtype=float32)\n",
      "embedding_first shape is (?, 24, 1)\n",
      "first_order: (?, 24)\n",
      "sum_square_second_order: Tensor(\"sec_lstm_2/Square:0\", shape=(?, 5), dtype=float32)\n",
      "square_sum_second_order: Tensor(\"sec_lstm_2/Sum_2:0\", shape=(?, 5), dtype=float32)\n",
      "fm_part: Tensor(\"sec_lstm_2/concat:0\", shape=(?, 29), dtype=float32)\n",
      "fm_part shape is (?, 10, 29)\n",
      "embedding shape is (?, 10, 120)\n",
      "continuous_part shape is (?, 10, 19)\n",
      ".........din_all shape is (?, 10, 168)\n",
      "..........din_all shape is (?, 10, 168)\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda4452e240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda4452e240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda4452e240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda4452e240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "(1, 10, 5)\n",
      ".......output shape (1, 5)\n",
      ":::::::::final_states shape is LSTMStateTuple(c=<tf.Tensor 'sec_lstm_2/rnn/while/Exit_3:0' shape=(1, 5) dtype=float32>, h=<tf.Tensor 'sec_lstm_2/rnn/while/Exit_4:0' shape=(1, 5) dtype=float32>)\n",
      "----w_out shape is (5, 1)\n",
      "----b_out shape is (1,)\n",
      "............weights out shape is (5, 1)\n",
      "------pred shape is  (1, 1)\n",
      "INFO:tensorflow:Restoring parameters from model_save1/modle.ckpt\n",
      "[array([0.00140312], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "prediction(test_value,test_index,con_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
